{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa1a47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "import math\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "method = 'Qwen2_5_0_5B_base'\n",
    "safetensors_path_act = f'saved_tensor/{method}/activation.safetensors'\n",
    "state_dict_qwt_input = load_file(safetensors_path_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "675ea85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.input torch.Size([1, 2048, 896])\n",
      "layers.1.input torch.Size([1, 2048, 896])\n",
      "layers.10.input torch.Size([1, 2048, 896])\n",
      "layers.11.input torch.Size([1, 2048, 896])\n",
      "layers.12.input torch.Size([1, 2048, 896])\n",
      "layers.13.input torch.Size([1, 2048, 896])\n",
      "layers.14.input torch.Size([1, 2048, 896])\n",
      "layers.15.input torch.Size([1, 2048, 896])\n",
      "layers.16.input torch.Size([1, 2048, 896])\n",
      "layers.17.input torch.Size([1, 2048, 896])\n",
      "layers.18.input torch.Size([1, 2048, 896])\n",
      "layers.19.input torch.Size([1, 2048, 896])\n",
      "layers.2.input torch.Size([1, 2048, 896])\n",
      "layers.20.input torch.Size([1, 2048, 896])\n",
      "layers.21.input torch.Size([1, 2048, 896])\n",
      "layers.22.input torch.Size([1, 2048, 896])\n",
      "layers.23.input torch.Size([1, 2048, 896])\n",
      "layers.3.input torch.Size([1, 2048, 896])\n",
      "layers.4.input torch.Size([1, 2048, 896])\n",
      "layers.5.input torch.Size([1, 2048, 896])\n",
      "layers.6.input torch.Size([1, 2048, 896])\n",
      "layers.7.input torch.Size([1, 2048, 896])\n",
      "layers.8.input torch.Size([1, 2048, 896])\n",
      "layers.9.input torch.Size([1, 2048, 896])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict_qwt_input.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 实例化MSELoss对象\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(state_dict_base.values(), state_dict_qwt_input.values()):\n",
    "    print(a.equal(b)) #, a.view(-1)[:5], b.view(-1)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('log/123.log', 'w') as f:\n",
    "    for idx, (a, b, c, d, e) in enumerate(zip(state_dict_base.values(), state_dict_quant.values(), state_dict_base.values(), state_dict_clamp.values(), state_dict_clamp_qwt.values())):\n",
    "        f.writelines(f'{idx} base-quant: {loss(a.float(), b.float()).item():>8.4f}, base-clamp: {loss(c.float(), d.float()).item():>8.4f}, base-qwt: {loss(a.float(), e.float()).item():>8.4f}\\n') # 11, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(22): \n",
    "    print(idx, state_dict_base[f'base_layers.{idx}.input'].equal(state_dict_qwt[f'qwt_layers.{idx}.input_full']))\n",
    "    # print(idx, loss(state_dict_base[f'base_layers.{idx}.input'].float(), state_dict_qwt[f'qwt_layers.{idx}.input_full'].float()))\n",
    "    # print('='*10)\n",
    "    # print(state_dict_base[f'base_layers.{idx}.input'].view(-1)[:5])\n",
    "    # print(state_dict_qwt[f'qwt_layers.{idx}.input_full'].view(-1)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_gpu_memory(tensor, name=None):\n",
    "    if tensor.is_cuda:\n",
    "        size_in_bytes = tensor.element_size() * tensor.numel()\n",
    "        size_in_MB = size_in_bytes / 1024**2\n",
    "        print(f\"{name if name else ''} size: {size_in_MB:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"{name if name else ''} is not on GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee23bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.empty((512, 2048, 4096), dtype=torch.float32).to('cuda')\n",
    "# print((4 * 512 * 2048 * 4096 * 32) / 1024 / 1024 / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to speedup debug, set length=512, n_sample=1, use opt-125m\n",
    "开源代码量化后的结果向中间结果做线性回归，此版本修改为向全精度对齐\n",
    "'''\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from tqdm import tqdm\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from globalVar import (get_save_tensor_enable,\n",
    "                       save_tensors,\n",
    "                       set_save_tensor_enable,\n",
    "                       set_data_type,\n",
    "                       append_activation,\n",
    "                       set_profiling_enable)\n",
    "\n",
    "def gather_tensor_from_multi_processes(input, world_size):\n",
    "    if world_size == 1:\n",
    "        return input\n",
    "    torch.cuda.synchronize()\n",
    "    dist.all_gather(gathered_tensors, input)\n",
    "    gathered_tensors = torch.cat(gathered_tensors, dim=0)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    return gathered_tensors\n",
    "def lienar_regression(X, Y, block_id=0):\n",
    "    X = X.reshape(-1, X.size(-1)).float()\n",
    "\n",
    "    X = gather_tensor_from_multi_processes(X, world_size=args.world_size)\n",
    "\n",
    "    X_add_one = torch.cat([X, torch.ones(size=[X.size(0), ], device=X.device).reshape(-1, 1)], dim=-1)\n",
    "    Y = Y.reshape(-1, Y.size(-1)).float()\n",
    "\n",
    "    Y = gather_tensor_from_multi_processes(Y, world_size=args.world_size)\n",
    "\n",
    "    # _write('the shape of X_add_one is {}, Y is {}'.format(X_add_one.size(), Y.size()))\n",
    "\n",
    "    X_add_one_T = X_add_one.t()\n",
    "    W_overall = torch.inverse(X_add_one_T @ X_add_one) @ X_add_one_T @ Y\n",
    "\n",
    "    W = W_overall[:-1, :]\n",
    "    b = W_overall[-1, :]\n",
    "\n",
    "    Y_pred = X @ W + b\n",
    "\n",
    "    abs_loss = (Y - Y_pred).abs().mean()\n",
    "\n",
    "    ss_tot = torch.sum((Y - Y.mean(dim=0)).pow(2))\n",
    "    ss_res = torch.sum((Y - Y_pred).pow(2))\n",
    "    r2_score = 1 - ss_res / ss_tot\n",
    "    # _write('block : {}      abs : {:.6f}      r2 : {:.3f}'.format(block_id, abs_loss, r2_score))\n",
    "    return W, b, r2_score\n",
    "def tensor_gpu_memory(tensor, name=None):\n",
    "    if tensor.is_cuda:\n",
    "        size_in_bytes = tensor.element_size() * tensor.numel()\n",
    "        size_in_MB = size_in_bytes / 1024**2\n",
    "        print(f\"{name if name else ''} size: {size_in_MB:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"{name if name else ''} is not on GPU.\")\n",
    "# current date and time\n",
    "current_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "start_time = time.time()\n",
    "# ----------------------------------------------------------\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--alpha\", type=float, default=0.5)\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"opt-125m\")\n",
    "parser.add_argument(\n",
    "    \"--act_scales_path\",\n",
    "    type=str,\n",
    "    default=\"act_scales/llama-2-7b.pt\",\n",
    ")\n",
    "parser.add_argument(\"--n_samples\", type=int, default=None)\n",
    "parser.add_argument(\"--device\", type=str, default='cuda')\n",
    "parser.add_argument('--start_block', default=0, type=int)\n",
    "parser.add_argument(\"--local-rank\", default=0, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "parser.add_argument(\"--num_workers\", default=4, type=int)\n",
    "parser.add_argument(\"--eval_base\", action=\"store_true\")\n",
    "parser.add_argument(\"--eval_quant\", action=\"store_true\")\n",
    "parser.add_argument(\"--eval_clamp\", action=\"store_true\")\n",
    "parser.add_argument(\"--eval_quant_qwt\", action=\"store_true\")\n",
    "parser.add_argument(\"--eval_clamp_qwt\", action=\"store_true\")\n",
    "parser.add_argument(\"--profiling\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        length = 2048\n",
    "        n_samples = self.n_samples if self.n_samples else self.dataset.size(1) // length\n",
    "        for i in tqdm(range(n_samples), desc=\"Evaluating...\"):\n",
    "            batch = self.dataset[:, (i * length) : ((i + 1) * length)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.dataset[:, (i * length) : ((i + 1) * length)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * length\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (n_samples * length))\n",
    "\n",
    "args.world_size = 1\n",
    "args.rank = 0  # global rank\n",
    "alpha = args.alpha\n",
    "model_path = '/localssd/lbxj/' + args.model_name\n",
    "# model_path = '/cephfs/juxin/models/' + args.model_name\n",
    "act_scales_path = args.act_scales_path\n",
    "n_samples = args.n_samples\n",
    "train_samples = 64 # 64\n",
    "# set_save_tensor_enable()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
